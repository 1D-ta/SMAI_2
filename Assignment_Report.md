# SMAI Assignment 2
# Traveling Salesman Problem (TSP) Report
### By Vandita Lodha and Suhani Jain

## Approach

Our approach to solving the TSP combines two heuristic methods:
1. Nearest Neighbor Algorithm
2. 2-opt Optimization

### Nearest Neighbor Algorithm

The Nearest Neighbor algorithm is used to generate an initial tour. It works as follows:

1. Start with an arbitrary city
2. Find the nearest unvisited city and add it to the tour
3. Repeat step 2 until all cities are visited

### 2-opt Optimization

To improve upon the initial tour generated by the Nearest Neighbor algorithm, we apply the 2-opt optimization technique. This method iteratively improves the tour by:

1. Considering all possible pairs of edges in the current tour
2. Swapping these edges if it results in a shorter tour
3. Repeating until no further improvements can be made

## Implementation
1. **Data Structures**: 
   - Cities are represented as complex numbers for easy Euclidean distance calculation
   - Tours are represented as lists of cities
   - A global dictionary `city_to_index` is used to map city coordinates to indices

2. **Key Functions**:
   - `nearest_neighbor`: Implements the Nearest Neighbor algorithm
   - `opt2`: Implements the 2-opt optimization
   - `rep_opt2_nearest_tsp`: Combines Nearest Neighbor and 2-opt, running multiple times with different starting cities

3. **Input Handling**:
   - The program can handle both Euclidean and non-Euclidean TSP instances
   - For non-Euclidean cases, the distance matrix is used

4. **Output**:
   - The program outputs the best tour found in the required format (zero-based city indices)

## Alternative Approaches

While our final implementation uses the Nearest Neighbor algorithm with 2-opt optimization, there are other approaches that we had explored to solve the TSP:

### 1. Simulated Annealing
Simulated Annealing is a probabilistic technique for approximating the global optimum of a given function. For TSP, it works as follows:

1. Start with a random tour
2. Repeatedly make small random changes to the tour
3. Accept changes that improve the tour length
4. Probabilistically accept some changes that worsen the tour length, with the probability decreasing over time

Pros:
- Can escape local optima
- Generally produces good solutions

Cons:
- Performance depends on cooling schedule
- Was slower than our approach for larger instances

### 2. Ant Colony Optimization
Ant Colony Optimization is inspired by the behavior of ants finding paths between their colony and food sources. For TSP, it works as follows:

1. Initialize pheromone trails
2. Construct tours for a colony of ants based on pheromone levels and distances
3. Update pheromone levels based on the quality of tours
4. Repeat steps 2-3 for a number of iterations

Pros:
- Can find high-quality solutions
- Inherently parallel

Cons:
- Requires careful parameter tuning
- Was computationally expensive for large instances

## Why Our Approach is Better

Our approach of combining Nearest Neighbor with 2-opt optimization offers several advantages:

1. **Speed**: Nearest Neighbor quickly generates an initial solution, while 2-opt provides efficient local improvements.
2. **Simplicity**: The algorithm is straightforward to implement and understand.
3. **Consistency**: By running multiple times with different starting points (200 in our implementation), we increase the chances of finding a good solution.
4. **Adaptability**: Our approach works well for both Euclidean and non-Euclidean instances without modification.
5. **Memory Efficiency**: Unlike population-based methods, our approach has minimal memory overhead.

While Simulated Annealing and Ant Colony Optimization can potentially find better solutions, they often require more computational resources and careful parameter tuning. Our method strikes a balance between solution quality and computational efficiency, making it well-suited for the constraints of this assignment.

In terms of time complexities, this is how the three methods look:

### Implemented Approach (Nearest Neighbor with 2-opt)

1. Nearest Neighbor (NN):
   - Time Complexity: O(n^2)
   - For each city, we search through all remaining cities to find the nearest one.

2. 2-opt Optimization:
   - Worst-case Time Complexity: O(n^2) per iteration
   - In the worst case, we might need to check all possible pairs of edges.
   - The number of iterations isn't fixed, but let's assume it's proportional to n for this analysis.

3. Overall Algorithm (rep_opt2_nearest_tsp):
   - Time Complexity: O(k * n^3), where k is the number of repetitions (200 in our code)
   - For each of k starts, we run NN (O(n^2)) and then 2-opt (O(n^3) assuming n iterations).

### Simulated Annealing (SA)

- Time Complexity: O(n^2 * T), where T is the number of iterations
- Each iteration involves:
  - Generating a neighbor solution: O(1)
  - Calculating the change in tour length: O(1)
  - Deciding whether to accept the new solution: O(1)
- The total number of iterations T is typically set as a function of n and the cooling schedule.
- In practice, T is often chosen to be n^2, resulting in an O(n^4) algorithm.

### Ant Colony Optimization (ACO)

- Time Complexity: O(N * n^2 * m), where:
  - N is the number of iterations
  - n is the number of cities
  - m is the number of ants
- In each iteration:
  - Each ant constructs a tour: O(n^2)
  - Update pheromone levels: O(n^2)
- Typically, m is chosen to be equal to n, giving a time complexity of O(N * n^3)

### Comparison

1. Our Approach: O(k * n^3)
   - Predictable performance
   - Scales cubically with the number of cities
   - The constant factor k (number of restarts) can be adjusted for time/quality trade-off

2. Simulated Annealing: O(n^4) (typical implementation)
   - Can potentially find better solutions given enough time
   - Performance heavily depends on cooling schedule and initial temperature

3. Ant Colony Optimization: O(N * n^3)
   - Can find high-quality solutions for some problem instances
   - Performance depends on the number of iterations and number of ants
   - May require careful tuning of parameters

## Potential Improvements for the future

1. **Hybrid Approaches**: Combine our method with elements of Simulated Annealing or Ant Colony Optimization for potentially better results.
2. **Parallelization**: Utilize parallel processing to explore multiple tours simultaneously.
3. **Adaptive Parameters**: Dynamically adjust the number of restarts based on available time and problem size.
4. **Caching**: Implement caching mechanisms to avoid redundant distance calculations.
5. **Early Termination**: Add an early termination condition to stop optimization if improvements become marginal.

## Conclusion

The implemented solution provides a balance between solution quality and computational efficiency. By combining the quick initial tour generation of Nearest Neighbor with the improvement capabilities of 2-opt, we achieve reasonably good tours within the given time constraints. Our future work would focus on implementing methods to ensure further path optimisation.
